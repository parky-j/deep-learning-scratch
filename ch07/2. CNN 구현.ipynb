{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ddb29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.29900837810372\n",
      "=== epoch:1, train acc:0.109, test acc:0.103 ===\n",
      "train loss:2.2971011543841784\n",
      "train loss:2.2913917077831134\n",
      "train loss:2.285442745860207\n",
      "train loss:2.276773239922216\n",
      "train loss:2.269254858115588\n",
      "train loss:2.2554990423912686\n",
      "train loss:2.2159233535033795\n",
      "train loss:2.198090954865529\n",
      "train loss:2.158488422209808\n",
      "train loss:2.170433118132255\n",
      "train loss:2.082561432192511\n",
      "train loss:2.109259543072282\n",
      "train loss:2.019846890510645\n",
      "train loss:1.9491842427464388\n",
      "train loss:1.9077718375073496\n",
      "train loss:1.8522526045400654\n",
      "train loss:1.8626486482582538\n",
      "train loss:1.7144638055318397\n",
      "train loss:1.6380998652015202\n",
      "train loss:1.5844270303898813\n",
      "train loss:1.5174156392825995\n",
      "train loss:1.4090858425662214\n",
      "train loss:1.3200360089319347\n",
      "train loss:1.2619568629774367\n",
      "train loss:1.044168949883713\n",
      "train loss:1.085248089548562\n",
      "train loss:1.0350665414813482\n",
      "train loss:0.9060151865890655\n",
      "train loss:0.7857212960961903\n",
      "train loss:0.9262511221262245\n",
      "train loss:0.7674807449697757\n",
      "train loss:0.7526362492672134\n",
      "train loss:0.8114872098228383\n",
      "train loss:0.6777268889485213\n",
      "train loss:0.7402700203964397\n",
      "train loss:0.718731949805523\n",
      "train loss:0.6890787145165307\n",
      "train loss:0.6609116415134088\n",
      "train loss:0.6061468708918751\n",
      "train loss:0.6700950211545229\n",
      "train loss:0.5439713079956541\n",
      "train loss:0.7796021691521837\n",
      "train loss:0.6921159323438016\n",
      "train loss:0.7090393018116111\n",
      "train loss:0.3623999463404956\n",
      "train loss:0.6531617037960276\n",
      "train loss:0.6833731874818966\n",
      "train loss:0.5295518729925341\n",
      "train loss:0.41058212677044176\n",
      "train loss:0.6619932248700383\n",
      "=== epoch:2, train acc:0.807, test acc:0.778 ===\n",
      "train loss:0.6441656589764966\n",
      "train loss:0.4011455202769415\n",
      "train loss:0.5895127472977333\n",
      "train loss:0.46020295007018\n",
      "train loss:0.5248290705355083\n",
      "train loss:0.5860882957070284\n",
      "train loss:0.4208832951788497\n",
      "train loss:0.4423682996096079\n",
      "train loss:0.3619580392021525\n",
      "train loss:0.4333546461821011\n",
      "train loss:0.43812428509596735\n",
      "train loss:0.46242626933551734\n",
      "train loss:0.46252142238146887\n",
      "train loss:0.28641645530370885\n",
      "train loss:0.35398818346809835\n",
      "train loss:0.40151688619786036\n",
      "train loss:0.34957423815034877\n",
      "train loss:0.5090584195895734\n",
      "train loss:0.34328629561152674\n",
      "train loss:0.4202945467072472\n",
      "train loss:0.36992730077371083\n",
      "train loss:0.43126241743740157\n",
      "train loss:0.5033119545861373\n",
      "train loss:0.3345303033240786\n",
      "train loss:0.3588356156094594\n",
      "train loss:0.27091897753018457\n",
      "train loss:0.2558942981684209\n",
      "train loss:0.28032639133296067\n",
      "train loss:0.3307933695797209\n",
      "train loss:0.1964314989841973\n",
      "train loss:0.31888948845933207\n",
      "train loss:0.356154555778729\n",
      "train loss:0.3022766139101638\n",
      "train loss:0.34958717855785904\n",
      "train loss:0.3324150889385143\n",
      "train loss:0.2578812764877291\n",
      "train loss:0.31904320008043\n",
      "train loss:0.31576856957696864\n",
      "train loss:0.43597435868994955\n",
      "train loss:0.5099139070427688\n",
      "train loss:0.3306747352159604\n",
      "train loss:0.28910683952827365\n",
      "train loss:0.24699131973083235\n",
      "train loss:0.2678908512807949\n",
      "train loss:0.2554264673403137\n",
      "train loss:0.47744256584838163\n",
      "train loss:0.3833747665865043\n",
      "train loss:0.39086674833896606\n",
      "train loss:0.3199805138042442\n",
      "train loss:0.27597727308436587\n",
      "=== epoch:3, train acc:0.855, test acc:0.852 ===\n",
      "train loss:0.3090545014683903\n",
      "train loss:0.30040110627618355\n",
      "train loss:0.4779683614519088\n",
      "train loss:0.18038878508322667\n",
      "train loss:0.2514621956902097\n",
      "train loss:0.28713825087953493\n",
      "train loss:0.5373825578185255\n",
      "train loss:0.30954153693844133\n",
      "train loss:0.28134868709724015\n",
      "train loss:0.47420845689998187\n",
      "train loss:0.32625977306258896\n",
      "train loss:0.45261789531059826\n",
      "train loss:0.23767016670302563\n",
      "train loss:0.4367252717665827\n",
      "train loss:0.22310056378059742\n",
      "train loss:0.5057086654944524\n",
      "train loss:0.22582152078891396\n",
      "train loss:0.3499614746636725\n",
      "train loss:0.29411902412091295\n",
      "train loss:0.2629697709431209\n",
      "train loss:0.19857969854930438\n",
      "train loss:0.3623972583598618\n",
      "train loss:0.18531164430529604\n",
      "train loss:0.29351275098734697\n",
      "train loss:0.32775934540817914\n",
      "train loss:0.24703433335062158\n",
      "train loss:0.5247082025192603\n",
      "train loss:0.21339388612971313\n",
      "train loss:0.34830379707704423\n",
      "train loss:0.28006699747313385\n",
      "train loss:0.2706226833693026\n",
      "train loss:0.37983600963822545\n",
      "train loss:0.22027321973023162\n",
      "train loss:0.24213675660809048\n",
      "train loss:0.2934270650079308\n",
      "train loss:0.29918061714021915\n",
      "train loss:0.34198712507999735\n",
      "train loss:0.2547695169551034\n",
      "train loss:0.2625211510189859\n",
      "train loss:0.3132015720480545\n",
      "train loss:0.39473037223665774\n",
      "train loss:0.21521356742970998\n",
      "train loss:0.3490323209065538\n",
      "train loss:0.2358100284053945\n",
      "train loss:0.2827647598813698\n",
      "train loss:0.2353091780814149\n",
      "train loss:0.43524997490851614\n",
      "train loss:0.22618766274186658\n",
      "train loss:0.3144627971282084\n",
      "train loss:0.3047295841806854\n",
      "=== epoch:4, train acc:0.896, test acc:0.885 ===\n",
      "train loss:0.32087038054031114\n",
      "train loss:0.27063399304143176\n",
      "train loss:0.30386172233897485\n",
      "train loss:0.3531635816481132\n",
      "train loss:0.3167935921938137\n",
      "train loss:0.19817072142004574\n",
      "train loss:0.2751637846541319\n",
      "train loss:0.30135184710110136\n",
      "train loss:0.15184332329723477\n",
      "train loss:0.2353696112405901\n",
      "train loss:0.2984765691478867\n",
      "train loss:0.2983873586836113\n",
      "train loss:0.17349039351736717\n",
      "train loss:0.22514398740915492\n",
      "train loss:0.19549332454146706\n",
      "train loss:0.2719219575808191\n",
      "train loss:0.33332335544431474\n",
      "train loss:0.30028420513479814\n",
      "train loss:0.35908235463946414\n",
      "train loss:0.3347544272422415\n",
      "train loss:0.5399159775750768\n",
      "train loss:0.19773115934463095\n",
      "train loss:0.23853277531217817\n",
      "train loss:0.2837770051391425\n",
      "train loss:0.16351422414387456\n",
      "train loss:0.29375974991330234\n",
      "train loss:0.2229028907142023\n",
      "train loss:0.4500294890498164\n",
      "train loss:0.19669966646220388\n",
      "train loss:0.3275423927181549\n",
      "train loss:0.30767071276552105\n",
      "train loss:0.3266362678884074\n",
      "train loss:0.2775844447495468\n",
      "train loss:0.34618219348676066\n",
      "train loss:0.23716153824996822\n",
      "train loss:0.19794719237026634\n",
      "train loss:0.19617126888167843\n",
      "train loss:0.27353405394852204\n",
      "train loss:0.2474069226775052\n",
      "train loss:0.2200601401307983\n",
      "train loss:0.2511115403562091\n",
      "train loss:0.19470461574611161\n",
      "train loss:0.19614972990049842\n",
      "train loss:0.31228124790214207\n",
      "train loss:0.25171436829150384\n",
      "train loss:0.18483204742241935\n",
      "train loss:0.19532117148699352\n",
      "train loss:0.16077583267138415\n",
      "train loss:0.24558166506832702\n",
      "train loss:0.25471670779972905\n",
      "=== epoch:5, train acc:0.907, test acc:0.904 ===\n",
      "train loss:0.13071434523915212\n",
      "train loss:0.26720605912269124\n",
      "train loss:0.19864787822712715\n",
      "train loss:0.33201154672579763\n",
      "train loss:0.31163303519927105\n",
      "train loss:0.3888245488915804\n",
      "train loss:0.24905434550894068\n",
      "train loss:0.32024453979309464\n",
      "train loss:0.24011068729281873\n",
      "train loss:0.21512959088817787\n",
      "train loss:0.13487367867425568\n",
      "train loss:0.27900875207871734\n",
      "train loss:0.17435727872428355\n",
      "train loss:0.4102287998123919\n",
      "train loss:0.3068729579722589\n",
      "train loss:0.2689563246120494\n",
      "train loss:0.16735472205089574\n",
      "train loss:0.17986332534223431\n",
      "train loss:0.17097731052330647\n",
      "train loss:0.16732743540005943\n",
      "train loss:0.18743732735540708\n",
      "train loss:0.1391802135597234\n",
      "train loss:0.14021192444108263\n",
      "train loss:0.23190858837689488\n",
      "train loss:0.28807681008779507\n",
      "train loss:0.22619457371053164\n",
      "train loss:0.10228768913942136\n",
      "train loss:0.16311995829683623\n",
      "train loss:0.26035660765742324\n",
      "train loss:0.32728965900856904\n",
      "train loss:0.27894990587943075\n",
      "train loss:0.25322633977800685\n",
      "train loss:0.216783682034267\n",
      "train loss:0.14871017150393093\n",
      "train loss:0.24981454558253527\n",
      "train loss:0.3466207677438807\n",
      "train loss:0.1448160339937188\n",
      "train loss:0.2941567177489762\n",
      "train loss:0.1034890320459226\n",
      "train loss:0.15217660560320437\n",
      "train loss:0.11385789580563324\n",
      "train loss:0.20606621832703065\n",
      "train loss:0.09485445639522638\n",
      "train loss:0.30852387730524844\n",
      "train loss:0.18984505652464267\n",
      "train loss:0.14267614536014017\n",
      "train loss:0.23929944597190952\n",
      "train loss:0.11848035347163027\n",
      "train loss:0.13945382067843456\n",
      "train loss:0.1641826474724742\n",
      "=== epoch:6, train acc:0.927, test acc:0.911 ===\n",
      "train loss:0.14696915004868247\n",
      "train loss:0.1836210954581761\n",
      "train loss:0.18077374541279773\n",
      "train loss:0.14335613142823436\n",
      "train loss:0.1576546870095914\n",
      "train loss:0.11442876998797863\n",
      "train loss:0.17242629923326466\n",
      "train loss:0.20320916115465323\n",
      "train loss:0.2551173238486518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.26171133786680184\n",
      "train loss:0.18805414290435643\n",
      "train loss:0.19469051195626508\n",
      "train loss:0.24643631817270528\n",
      "train loss:0.23950653589407128\n",
      "train loss:0.10799001445343989\n",
      "train loss:0.2889640238565781\n",
      "train loss:0.267942144800645\n",
      "train loss:0.2654473204950594\n",
      "train loss:0.17077183272999114\n",
      "train loss:0.2302359680260977\n",
      "train loss:0.15484217470653522\n",
      "train loss:0.24784540778182482\n",
      "train loss:0.1422869508563791\n",
      "train loss:0.33225321570895877\n",
      "train loss:0.2287580608956485\n",
      "train loss:0.1981425422133207\n",
      "train loss:0.1694551179862516\n",
      "train loss:0.14668045746471722\n",
      "train loss:0.1799069058409\n",
      "train loss:0.1929570560192904\n",
      "train loss:0.22024887389398454\n",
      "train loss:0.14636367271313996\n",
      "train loss:0.2705035100409441\n",
      "train loss:0.2887808194689497\n",
      "train loss:0.18116991513174216\n",
      "train loss:0.13120872627035532\n",
      "train loss:0.16357766391199666\n",
      "train loss:0.17572714599900047\n",
      "train loss:0.16236580806280634\n",
      "train loss:0.2000546807257292\n",
      "train loss:0.14331980906415828\n",
      "train loss:0.29981862419911953\n",
      "train loss:0.14543727078414503\n",
      "train loss:0.174948903391038\n",
      "train loss:0.08351660570168472\n",
      "train loss:0.21206132573437259\n",
      "train loss:0.3060578348235198\n",
      "train loss:0.15420701498769118\n",
      "train loss:0.10651254854118909\n",
      "train loss:0.18988266721010028\n",
      "=== epoch:7, train acc:0.937, test acc:0.916 ===\n",
      "train loss:0.09455406000311653\n",
      "train loss:0.1690872520784424\n",
      "train loss:0.11595082065467038\n",
      "train loss:0.1995489475281612\n",
      "train loss:0.11409160355679539\n",
      "train loss:0.2540502331377589\n",
      "train loss:0.22995088776650086\n",
      "train loss:0.1992650434006228\n",
      "train loss:0.23550871142019886\n",
      "train loss:0.23169519873774203\n",
      "train loss:0.13792333514884894\n",
      "train loss:0.17160004533065043\n",
      "train loss:0.15632329851354362\n",
      "train loss:0.1631727212676656\n",
      "train loss:0.11410277317675802\n",
      "train loss:0.1819472112839474\n",
      "train loss:0.16237269993716502\n",
      "train loss:0.142085702425537\n",
      "train loss:0.10592119528102223\n",
      "train loss:0.21292389589912197\n",
      "train loss:0.30875647549648033\n",
      "train loss:0.19345484302185384\n",
      "train loss:0.18420712629871022\n",
      "train loss:0.23135756753834444\n",
      "train loss:0.210016542710279\n",
      "train loss:0.0720362214559998\n",
      "train loss:0.1579031511480003\n",
      "train loss:0.15520588564072343\n",
      "train loss:0.13333955252628346\n",
      "train loss:0.12781507995317873\n",
      "train loss:0.1926106520804933\n",
      "train loss:0.16941976153279065\n",
      "train loss:0.2047139767214889\n",
      "train loss:0.1537396305037869\n",
      "train loss:0.13569281436705385\n",
      "train loss:0.1803539393291201\n",
      "train loss:0.17833443578960007\n",
      "train loss:0.15496684158435894\n",
      "train loss:0.310549197658938\n",
      "train loss:0.18999884306818646\n",
      "train loss:0.18794643337259687\n",
      "train loss:0.08191173927613328\n",
      "train loss:0.0386646553905493\n",
      "train loss:0.1317370632560281\n",
      "train loss:0.13399168766623876\n",
      "train loss:0.17230872475709003\n",
      "train loss:0.17296924981037848\n",
      "train loss:0.11163489988139559\n",
      "train loss:0.10287664085013679\n",
      "train loss:0.21418627175131305\n",
      "=== epoch:8, train acc:0.928, test acc:0.904 ===\n",
      "train loss:0.12014805844499547\n",
      "train loss:0.13896759397420405\n",
      "train loss:0.1297099735225189\n",
      "train loss:0.17004468244553203\n",
      "train loss:0.0932945093198231\n",
      "train loss:0.10975137916830711\n",
      "train loss:0.23653900538197972\n",
      "train loss:0.06862244244976537\n",
      "train loss:0.08650610729041242\n",
      "train loss:0.23347051564914015\n",
      "train loss:0.12095023636278844\n",
      "train loss:0.17329905220746095\n",
      "train loss:0.2905268350922094\n",
      "train loss:0.14149393264238674\n",
      "train loss:0.1116394771841109\n",
      "train loss:0.13271931300617215\n",
      "train loss:0.09029854786968455\n",
      "train loss:0.12470152904991187\n",
      "train loss:0.0848900639440609\n",
      "train loss:0.15725097112617875\n",
      "train loss:0.11189821183122692\n",
      "train loss:0.15884901631593118\n",
      "train loss:0.0947321806104174\n",
      "train loss:0.14138923898509395\n",
      "train loss:0.12035319160517328\n",
      "train loss:0.09056512095548358\n",
      "train loss:0.12526206394770703\n",
      "train loss:0.15411167317478755\n",
      "train loss:0.22020967966803762\n",
      "train loss:0.10314942046766587\n",
      "train loss:0.1083548073331063\n",
      "train loss:0.2073019822471266\n",
      "train loss:0.06437700430637093\n",
      "train loss:0.12183556420711693\n",
      "train loss:0.07445915616581053\n",
      "train loss:0.17729667435859325\n",
      "train loss:0.09768752684217764\n",
      "train loss:0.2029703938179807\n",
      "train loss:0.13869711261164488\n",
      "train loss:0.0939518773963544\n",
      "train loss:0.16204986896578066\n",
      "train loss:0.08579622507945761\n",
      "train loss:0.15880499210110918\n",
      "train loss:0.1726053585630126\n",
      "train loss:0.12741786968167448\n",
      "train loss:0.08192982159705667\n",
      "train loss:0.10974982881117212\n",
      "train loss:0.08356200369544683\n",
      "train loss:0.1194523353809145\n",
      "train loss:0.08746834023547118\n",
      "=== epoch:9, train acc:0.946, test acc:0.93 ===\n",
      "train loss:0.10430828453013699\n",
      "train loss:0.11577203901811477\n",
      "train loss:0.25859795611203906\n",
      "train loss:0.10918288930089905\n",
      "train loss:0.10495559489660763\n",
      "train loss:0.17717799019651742\n",
      "train loss:0.0793924906463081\n",
      "train loss:0.08945734688048361\n",
      "train loss:0.07819616856835827\n",
      "train loss:0.08845872534439236\n",
      "train loss:0.10457960951515315\n",
      "train loss:0.14062073991136811\n",
      "train loss:0.1056912237349025\n",
      "train loss:0.08151880445585691\n",
      "train loss:0.05354845558671646\n",
      "train loss:0.09905631913673602\n",
      "train loss:0.10584519818559718\n",
      "train loss:0.1673912138677585\n",
      "train loss:0.08391496651315468\n",
      "train loss:0.1180527518357173\n",
      "train loss:0.13090875793200263\n",
      "train loss:0.14772556678872906\n",
      "train loss:0.22149171642381407\n",
      "train loss:0.06099145659903263\n",
      "train loss:0.1888955540130511\n",
      "train loss:0.04158013858998632\n",
      "train loss:0.0593610311419938\n",
      "train loss:0.0996752956242744\n",
      "train loss:0.18944946209067928\n",
      "train loss:0.09019413772177001\n",
      "train loss:0.07018089100786301\n",
      "train loss:0.13104801413501335\n",
      "train loss:0.08215859493400016\n",
      "train loss:0.03854856499493669\n",
      "train loss:0.08836531166284267\n",
      "train loss:0.13601738851041414\n",
      "train loss:0.12487311205608212\n",
      "train loss:0.07863124543385569\n",
      "train loss:0.13517523602365986\n",
      "train loss:0.1189484213327592\n",
      "train loss:0.12463235151812808\n",
      "train loss:0.05965219302325212\n",
      "train loss:0.11013441652000815\n",
      "train loss:0.12479877301854345\n",
      "train loss:0.1679044524240319\n",
      "train loss:0.1323173160326994\n",
      "train loss:0.189410124601115\n",
      "train loss:0.0761200870702599\n",
      "train loss:0.14338543830201358\n",
      "train loss:0.13518300386223994\n",
      "=== epoch:10, train acc:0.961, test acc:0.942 ===\n",
      "train loss:0.14342752983668378\n",
      "train loss:0.0946657291057168\n",
      "train loss:0.09433249257071072\n",
      "train loss:0.07450468333782084\n",
      "train loss:0.08482096016287477\n",
      "train loss:0.15660790452158874\n",
      "train loss:0.09601305363664304\n",
      "train loss:0.20001381844779853\n",
      "train loss:0.09926551026699303\n",
      "train loss:0.10885937834463653\n",
      "train loss:0.09678911180588096\n",
      "train loss:0.08473229107851131\n",
      "train loss:0.07087437970431792\n",
      "train loss:0.1852089463513521\n",
      "train loss:0.22161680809535184\n",
      "train loss:0.10812989898861128\n",
      "train loss:0.046283795039479186\n",
      "train loss:0.09278626394888624\n",
      "train loss:0.1076405996132611\n",
      "train loss:0.06938513611989018\n",
      "train loss:0.07829904488493541\n",
      "train loss:0.06256032144042709\n",
      "train loss:0.08563958474691612\n",
      "train loss:0.04076852503272857\n",
      "train loss:0.1970425207700471\n",
      "train loss:0.07212634568448349\n",
      "train loss:0.1539192147677352\n",
      "train loss:0.055622553351236\n",
      "train loss:0.05313229414469805\n",
      "train loss:0.1278232505077481\n",
      "train loss:0.0756862616108564\n",
      "train loss:0.09908254591490565\n",
      "train loss:0.11002817560514756\n",
      "train loss:0.043454587999597054\n",
      "train loss:0.09957436408309203\n",
      "train loss:0.0778365704403493\n",
      "train loss:0.07213579394764304\n",
      "train loss:0.0768360473311407\n",
      "train loss:0.02832296471805051\n",
      "train loss:0.09339347087866608\n",
      "train loss:0.08430409695687587\n",
      "train loss:0.09610444208551477\n",
      "train loss:0.13674671399448884\n",
      "train loss:0.10762996225750715\n",
      "train loss:0.1070153709774116\n",
      "train loss:0.025665487558328738\n",
      "train loss:0.11134169406980972\n",
      "train loss:0.05659090410908123\n",
      "train loss:0.15439150994587605\n",
      "train loss:0.05158792393932659\n",
      "=== epoch:11, train acc:0.958, test acc:0.94 ===\n",
      "train loss:0.20813126429997963\n",
      "train loss:0.043119987926519644\n",
      "train loss:0.10152869649795894\n",
      "train loss:0.07554471970821411\n",
      "train loss:0.12344950039008466\n",
      "train loss:0.05446218563211264\n",
      "train loss:0.12821559383631734\n",
      "train loss:0.14726001027652205\n",
      "train loss:0.08836449260947284\n",
      "train loss:0.06303304285878655\n",
      "train loss:0.08461662634290618\n",
      "train loss:0.08451725462215964\n",
      "train loss:0.06504714251375493\n",
      "train loss:0.027995579300414214\n",
      "train loss:0.10293604150765077\n",
      "train loss:0.07815561008937173\n",
      "train loss:0.10536094744505516\n",
      "train loss:0.08026863467802561\n",
      "train loss:0.13977280375926776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.07936170220276775\n",
      "train loss:0.10470997482244225\n",
      "train loss:0.1025292886941204\n",
      "train loss:0.05678347356356169\n",
      "train loss:0.07056305646438073\n",
      "train loss:0.07457252970034173\n",
      "train loss:0.0773330667326101\n",
      "train loss:0.05780394488366885\n",
      "train loss:0.05489553293091109\n",
      "train loss:0.12167123890696692\n",
      "train loss:0.0600267829828858\n",
      "train loss:0.17895729021339743\n",
      "train loss:0.03489482342855852\n",
      "train loss:0.05836310180245662\n",
      "train loss:0.06177045213094305\n",
      "train loss:0.11666221083175748\n",
      "train loss:0.11226123321011539\n",
      "train loss:0.0846247787214635\n",
      "train loss:0.0977306336612779\n",
      "train loss:0.09235899865167915\n",
      "train loss:0.05714679511923834\n",
      "train loss:0.10606765781327893\n",
      "train loss:0.059292582346811756\n",
      "train loss:0.07086715231389579\n",
      "train loss:0.0597559042284022\n",
      "train loss:0.0764592346462047\n",
      "train loss:0.04797447470442583\n",
      "train loss:0.0445490525974282\n",
      "train loss:0.03427302701012655\n",
      "train loss:0.050150475916491466\n",
      "train loss:0.11896027371370527\n",
      "=== epoch:12, train acc:0.965, test acc:0.943 ===\n",
      "train loss:0.041716005174277454\n",
      "train loss:0.04482205479535471\n",
      "train loss:0.06565479741599102\n",
      "train loss:0.08692956033701217\n",
      "train loss:0.07815890656866586\n",
      "train loss:0.03704790899107688\n",
      "train loss:0.07199179515910648\n",
      "train loss:0.0521489380845651\n",
      "train loss:0.06860431119031202\n",
      "train loss:0.11392915119854419\n",
      "train loss:0.08737232672101516\n",
      "train loss:0.054508185757198714\n",
      "train loss:0.07415098558334601\n",
      "train loss:0.07568427285418954\n",
      "train loss:0.15693067482277243\n",
      "train loss:0.07143790300458562\n",
      "train loss:0.05394808939996283\n",
      "train loss:0.1254294452763647\n",
      "train loss:0.054649444827716366\n",
      "train loss:0.01900506677584065\n",
      "train loss:0.09752568867419037\n",
      "train loss:0.0860732455646051\n",
      "train loss:0.10091348966717126\n",
      "train loss:0.04346241041332802\n",
      "train loss:0.06902805190325455\n",
      "train loss:0.04556327322834347\n",
      "train loss:0.06072782383085522\n",
      "train loss:0.05355432906020228\n",
      "train loss:0.06649618948599663\n",
      "train loss:0.08107750877744303\n",
      "train loss:0.08359164463068933\n",
      "train loss:0.046694256540186864\n",
      "train loss:0.1099507786847842\n",
      "train loss:0.078071743607447\n",
      "train loss:0.040714054828410534\n",
      "train loss:0.036852453267707926\n",
      "train loss:0.11139061820522998\n",
      "train loss:0.05150621849453216\n",
      "train loss:0.060406492118159016\n",
      "train loss:0.09656588493825868\n",
      "train loss:0.06430781551329154\n",
      "train loss:0.1088600610464604\n",
      "train loss:0.09184988146780183\n",
      "train loss:0.027053096346492344\n",
      "train loss:0.1104124622391095\n",
      "train loss:0.04084718837076018\n",
      "train loss:0.156257722436571\n",
      "train loss:0.13730240998660587\n",
      "train loss:0.05177974224883629\n",
      "train loss:0.08950468692020358\n",
      "=== epoch:13, train acc:0.968, test acc:0.94 ===\n",
      "train loss:0.04472170689806447\n",
      "train loss:0.053101961800710196\n",
      "train loss:0.0826649152416051\n",
      "train loss:0.03740045588309084\n",
      "train loss:0.1507255584565142\n",
      "train loss:0.06923411498704016\n",
      "train loss:0.09551816139614801\n",
      "train loss:0.060924308702876656\n",
      "train loss:0.1585091192157375\n",
      "train loss:0.10511677982990195\n",
      "train loss:0.0417802114633877\n",
      "train loss:0.047045882734719095\n",
      "train loss:0.04914245700703423\n",
      "train loss:0.04356584840195124\n",
      "train loss:0.07173925168459858\n",
      "train loss:0.11743754228403772\n",
      "train loss:0.1254137758568188\n",
      "train loss:0.096507700782252\n",
      "train loss:0.040431992207719\n",
      "train loss:0.0435741049719587\n",
      "train loss:0.08552084613120041\n",
      "train loss:0.05994613033550559\n",
      "train loss:0.06653129357260973\n",
      "train loss:0.0385400715169046\n",
      "train loss:0.1275028055224765\n",
      "train loss:0.04435316169205581\n",
      "train loss:0.037252180813105745\n",
      "train loss:0.11520074080250783\n",
      "train loss:0.14405272311247802\n",
      "train loss:0.041003557563062225\n",
      "train loss:0.04132840791836794\n",
      "train loss:0.02785547643461268\n",
      "train loss:0.04973863854340349\n",
      "train loss:0.08646334766382269\n",
      "train loss:0.029543165666687313\n",
      "train loss:0.06724754217976897\n",
      "train loss:0.04098896320095764\n",
      "train loss:0.08326261302838255\n",
      "train loss:0.08520007638407863\n",
      "train loss:0.027242213706487873\n",
      "train loss:0.05966600536514604\n",
      "train loss:0.04194384169983568\n",
      "train loss:0.0641619226811446\n",
      "train loss:0.06532634136441595\n",
      "train loss:0.05632035494196193\n",
      "train loss:0.07211019281438534\n",
      "train loss:0.049240102283179696\n",
      "train loss:0.0688588211218312\n",
      "train loss:0.02287039613608782\n",
      "train loss:0.02513150946220098\n",
      "=== epoch:14, train acc:0.977, test acc:0.961 ===\n",
      "train loss:0.05101666769984228\n",
      "train loss:0.019118375314460787\n",
      "train loss:0.057039105781247317\n",
      "train loss:0.06842167383527269\n",
      "train loss:0.07295362565474935\n",
      "train loss:0.0601343600356743\n",
      "train loss:0.03381254380809242\n",
      "train loss:0.047951108921170514\n",
      "train loss:0.03779435412003176\n",
      "train loss:0.03865171821874863\n",
      "train loss:0.1179923970603378\n",
      "train loss:0.022605241878210065\n",
      "train loss:0.03307871581277025\n",
      "train loss:0.029047875704595723\n",
      "train loss:0.07745156686030434\n",
      "train loss:0.02290017351853303\n",
      "train loss:0.024087940212412343\n",
      "train loss:0.03368955050212339\n",
      "train loss:0.14717991659144275\n",
      "train loss:0.13469686047485646\n",
      "train loss:0.07701959055066135\n",
      "train loss:0.020432964016133835\n",
      "train loss:0.06290203677258324\n",
      "train loss:0.05080358117499911\n",
      "train loss:0.09367881859661795\n",
      "train loss:0.0700890497303196\n",
      "train loss:0.10741341192493385\n",
      "train loss:0.035468366903158605\n",
      "train loss:0.0885098602379554\n",
      "train loss:0.07806679501043283\n",
      "train loss:0.04345837491915651\n",
      "train loss:0.10754660645992815\n",
      "train loss:0.05903776208891794\n",
      "train loss:0.036325293902213514\n",
      "train loss:0.03013282784841726\n",
      "train loss:0.025428495801714884\n",
      "train loss:0.10718928823668859\n",
      "train loss:0.07691407803068374\n",
      "train loss:0.10713824165162576\n",
      "train loss:0.022912616688217185\n",
      "train loss:0.06496380935790035\n",
      "train loss:0.13861584217558373\n",
      "train loss:0.023035242864043248\n",
      "train loss:0.07895520237856\n",
      "train loss:0.06774940301734421\n",
      "train loss:0.05929821058691322\n",
      "train loss:0.03217223596637955\n",
      "train loss:0.07603017305564594\n",
      "train loss:0.044137198751810415\n",
      "train loss:0.05545044772634908\n",
      "=== epoch:15, train acc:0.982, test acc:0.959 ===\n",
      "train loss:0.05298202751462682\n",
      "train loss:0.051678915675226006\n",
      "train loss:0.03052483743494564\n",
      "train loss:0.04007001677642166\n",
      "train loss:0.05651852421480995\n",
      "train loss:0.028498842745101314\n",
      "train loss:0.04576049230253563\n",
      "train loss:0.023677125010209868\n",
      "train loss:0.0437495540392148\n",
      "train loss:0.05593032936941569\n",
      "train loss:0.05231993952441472\n",
      "train loss:0.10178283508000964\n",
      "train loss:0.016335327531430716\n",
      "train loss:0.06745462332466291\n",
      "train loss:0.04291865745648018\n",
      "train loss:0.04711062564357264\n",
      "train loss:0.027726102914630058\n",
      "train loss:0.03288271685773384\n",
      "train loss:0.049998687504775374\n",
      "train loss:0.04265985764718362\n",
      "train loss:0.024098163600311705\n",
      "train loss:0.030711397543431166\n",
      "train loss:0.053122434639520544\n",
      "train loss:0.02418171571008923\n",
      "train loss:0.06787944121842761\n",
      "train loss:0.04453430544244662\n",
      "train loss:0.05317113518518592\n",
      "train loss:0.020119814125081306\n",
      "train loss:0.04132779508460798\n",
      "train loss:0.09803335367194897\n",
      "train loss:0.028676456121240243\n",
      "train loss:0.07299801440719864\n",
      "train loss:0.04225648231203911\n",
      "train loss:0.04077865520388255\n",
      "train loss:0.03923581622588239\n",
      "train loss:0.020782869851809434\n",
      "train loss:0.14462469612117942\n",
      "train loss:0.05703946984728274\n",
      "train loss:0.04921748198825905\n",
      "train loss:0.035251949186295144\n",
      "train loss:0.031583800702504466\n",
      "train loss:0.02592935016193152\n",
      "train loss:0.037628586185367234\n",
      "train loss:0.014408434873719553\n",
      "train loss:0.13640157232832592\n",
      "train loss:0.03962538815628471\n",
      "train loss:0.042791918378011544\n",
      "train loss:0.04216542901467031\n",
      "train loss:0.05725321117492649\n",
      "train loss:0.04321814842445387\n",
      "=== epoch:16, train acc:0.981, test acc:0.959 ===\n",
      "train loss:0.044065084048716716\n",
      "train loss:0.05575467487402939\n",
      "train loss:0.03237499422555551\n",
      "train loss:0.0889387057321009\n",
      "train loss:0.0595628327109608\n",
      "train loss:0.017278560712039873\n",
      "train loss:0.09023832630835477\n",
      "train loss:0.02527112776555704\n",
      "train loss:0.025180372125577364\n",
      "train loss:0.020788123243091823\n",
      "train loss:0.04130391039259946\n",
      "train loss:0.025422443883403787\n",
      "train loss:0.010129978929917256\n",
      "train loss:0.08690531711691538\n",
      "train loss:0.020716766650295444\n",
      "train loss:0.038186238623776576\n",
      "train loss:0.05817711118626006\n",
      "train loss:0.048505230608419554\n",
      "train loss:0.01694400642723412\n",
      "train loss:0.0324340472027933\n",
      "train loss:0.04718512440229405\n",
      "train loss:0.05193015101202161\n",
      "train loss:0.010756691044150049\n",
      "train loss:0.027682224867082557\n",
      "train loss:0.023158580945145807\n",
      "train loss:0.029879254163185247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.017750694661846027\n",
      "train loss:0.02515514162175075\n",
      "train loss:0.04232131455444763\n",
      "train loss:0.022489966805566582\n",
      "train loss:0.0337899587278569\n",
      "train loss:0.021110018365646074\n",
      "train loss:0.00863749110607625\n",
      "train loss:0.047958341609327854\n",
      "train loss:0.03500509567431163\n",
      "train loss:0.024894746116958077\n",
      "train loss:0.022709484062867335\n",
      "train loss:0.02287311597689393\n",
      "train loss:0.04027252141498546\n",
      "train loss:0.033739372349163514\n",
      "train loss:0.01896532739313461\n",
      "train loss:0.045595275669640965\n",
      "train loss:0.06767563817648872\n",
      "train loss:0.018238063120035224\n",
      "train loss:0.0317391556429993\n",
      "train loss:0.04397296678003498\n",
      "train loss:0.0382418137265786\n",
      "train loss:0.10988113210949121\n",
      "train loss:0.04300564666235829\n",
      "train loss:0.03175641716570953\n",
      "=== epoch:17, train acc:0.988, test acc:0.953 ===\n",
      "train loss:0.04909183848022389\n",
      "train loss:0.026197632945063224\n",
      "train loss:0.15965236626350224\n",
      "train loss:0.015871805782305633\n",
      "train loss:0.030486287072180568\n",
      "train loss:0.028565444379374753\n",
      "train loss:0.02878421367709637\n",
      "train loss:0.02734408238394548\n",
      "train loss:0.03210945813492312\n",
      "train loss:0.07691997594220243\n",
      "train loss:0.03210599609159747\n",
      "train loss:0.027048914769762362\n",
      "train loss:0.022501420181195747\n",
      "train loss:0.05793700188023414\n",
      "train loss:0.02756243122157055\n",
      "train loss:0.035693455364790704\n",
      "train loss:0.01330229083664507\n",
      "train loss:0.019707167025060913\n",
      "train loss:0.02782166917935446\n",
      "train loss:0.009791220095688016\n",
      "train loss:0.02595272723281585\n",
      "train loss:0.009938255690829434\n",
      "train loss:0.025833104023334635\n",
      "train loss:0.02209476517381018\n",
      "train loss:0.01807897241821857\n",
      "train loss:0.012031972539892846\n",
      "train loss:0.048457472751635144\n",
      "train loss:0.018873321693878738\n",
      "train loss:0.015396866903292627\n",
      "train loss:0.028688318769967178\n",
      "train loss:0.028832396063164536\n",
      "train loss:0.028949175960394872\n",
      "train loss:0.018913669834383234\n",
      "train loss:0.011217782336566728\n",
      "train loss:0.013920953172945436\n",
      "train loss:0.023053103860733462\n",
      "train loss:0.020347587709048525\n",
      "train loss:0.015610127478024259\n",
      "train loss:0.020259048338242925\n",
      "train loss:0.010491038418200007\n",
      "train loss:0.0394722046922602\n",
      "train loss:0.023110073085251245\n",
      "train loss:0.032382372665965636\n",
      "train loss:0.02034447427608429\n",
      "train loss:0.023862628885482136\n",
      "train loss:0.034428543999941524\n",
      "train loss:0.09025798126590072\n",
      "train loss:0.012943448870327001\n",
      "train loss:0.035299493014482895\n",
      "train loss:0.11529670946271472\n",
      "=== epoch:18, train acc:0.99, test acc:0.958 ===\n",
      "train loss:0.026654168767659994\n",
      "train loss:0.07844322423540732\n",
      "train loss:0.018612861063990058\n",
      "train loss:0.01233321188026928\n",
      "train loss:0.01753264053131859\n",
      "train loss:0.01822353218182487\n",
      "train loss:0.038477549809786134\n",
      "train loss:0.0525607025478164\n",
      "train loss:0.016581682164951303\n",
      "train loss:0.02138608781643299\n",
      "train loss:0.03238950904260181\n",
      "train loss:0.02352841833054123\n",
      "train loss:0.014888385314450364\n",
      "train loss:0.035259011276707435\n",
      "train loss:0.019127721426626002\n",
      "train loss:0.019966684005104117\n",
      "train loss:0.06086690882335999\n",
      "train loss:0.029541185265928013\n",
      "train loss:0.02507556672475247\n",
      "train loss:0.029322789548138262\n",
      "train loss:0.053165923562717154\n",
      "train loss:0.029597200045262714\n",
      "train loss:0.06506908417560571\n",
      "train loss:0.053873844096525854\n",
      "train loss:0.03772426204539882\n",
      "train loss:0.040240757920880534\n",
      "train loss:0.016481283428952873\n",
      "train loss:0.02197312760563425\n",
      "train loss:0.039863398295434116\n",
      "train loss:0.06145568874390337\n",
      "train loss:0.017725635765807976\n",
      "train loss:0.022245236144665073\n",
      "train loss:0.03179612340264243\n",
      "train loss:0.013900652765797654\n",
      "train loss:0.019009123939046055\n",
      "train loss:0.040136767278535\n",
      "train loss:0.010615004314739648\n",
      "train loss:0.06571233731885788\n",
      "train loss:0.028057295351944837\n",
      "train loss:0.036041056324527775\n",
      "train loss:0.05328021248618149\n",
      "train loss:0.035064607415499396\n",
      "train loss:0.02054239561025374\n",
      "train loss:0.03775629933536873\n",
      "train loss:0.04346005623397341\n",
      "train loss:0.018642796484451152\n",
      "train loss:0.06687988718293096\n",
      "train loss:0.05462944718760067\n",
      "train loss:0.02148859763656447\n",
      "train loss:0.040544067221499794\n",
      "=== epoch:19, train acc:0.991, test acc:0.959 ===\n",
      "train loss:0.023904927957353238\n",
      "train loss:0.015967238803034148\n",
      "train loss:0.02844061831762172\n",
      "train loss:0.04069302056940456\n",
      "train loss:0.008574260900654818\n",
      "train loss:0.029620981514716812\n",
      "train loss:0.02665093530859851\n",
      "train loss:0.06570480065793595\n",
      "train loss:0.06926730503172959\n",
      "train loss:0.029527506472870838\n",
      "train loss:0.059911425260480836\n",
      "train loss:0.044949647125758434\n",
      "train loss:0.03773080779778534\n",
      "train loss:0.023756569538875505\n",
      "train loss:0.027438519841909936\n",
      "train loss:0.020723781566830312\n",
      "train loss:0.04593474388138841\n",
      "train loss:0.02431648350219262\n",
      "train loss:0.03885859359457728\n",
      "train loss:0.015718328818194995\n",
      "train loss:0.04513080669846491\n",
      "train loss:0.0455153648553382\n",
      "train loss:0.026622318293204307\n",
      "train loss:0.03130591217107187\n",
      "train loss:0.03431530475749724\n",
      "train loss:0.03514195916092607\n",
      "train loss:0.017174294473561943\n",
      "train loss:0.03419028588823492\n",
      "train loss:0.012696489802294235\n",
      "train loss:0.02043787971426632\n",
      "train loss:0.013976434295425161\n",
      "train loss:0.023580349122326715\n",
      "train loss:0.01404817147567849\n",
      "train loss:0.04146218804031682\n",
      "train loss:0.005860080495048873\n",
      "train loss:0.047512005217787416\n",
      "train loss:0.01402807314059783\n",
      "train loss:0.02417514873832204\n",
      "train loss:0.026671629981873904\n",
      "train loss:0.00972010068017869\n",
      "train loss:0.02499422029582442\n",
      "train loss:0.024612058530662125\n",
      "train loss:0.02317575487463515\n",
      "train loss:0.01732020105331135\n",
      "train loss:0.05885850994751908\n",
      "train loss:0.03387519053783393\n",
      "train loss:0.029178711249255595\n",
      "train loss:0.025944427232802877\n",
      "train loss:0.022057510906178742\n",
      "train loss:0.009670878268067128\n",
      "=== epoch:20, train acc:0.991, test acc:0.959 ===\n",
      "train loss:0.032210190016574276\n",
      "train loss:0.019996111795626527\n",
      "train loss:0.04680496314415168\n",
      "train loss:0.06341879391951535\n",
      "train loss:0.02410416921719137\n",
      "train loss:0.012993690148018633\n",
      "train loss:0.00400870604433834\n",
      "train loss:0.01893500419078831\n",
      "train loss:0.040580976429116955\n",
      "train loss:0.045563871810130135\n",
      "train loss:0.01835682193217694\n",
      "train loss:0.05180612030939771\n",
      "train loss:0.029895235470841428\n",
      "train loss:0.04763027810774445\n",
      "train loss:0.01408881187304281\n",
      "train loss:0.033326103409271404\n",
      "train loss:0.01388897260806083\n",
      "train loss:0.010099756317908572\n",
      "train loss:0.02584389938386914\n",
      "train loss:0.02449308344901454\n",
      "train loss:0.031298446993576576\n",
      "train loss:0.0311775751468637\n",
      "train loss:0.029899634008005164\n",
      "train loss:0.01824627127919335\n",
      "train loss:0.00830316530788021\n",
      "train loss:0.04979831684431264\n",
      "train loss:0.020493062474194867\n",
      "train loss:0.021513081923711902\n",
      "train loss:0.017075171459797377\n",
      "train loss:0.024052206565844544\n",
      "train loss:0.020002048627050618\n",
      "train loss:0.01219101732208555\n",
      "train loss:0.012696367302759129\n",
      "train loss:0.01737450597173955\n",
      "train loss:0.02757008033397506\n",
      "train loss:0.012987968897233854\n",
      "train loss:0.010041843599090088\n",
      "train loss:0.011125920636652456\n",
      "train loss:0.010634610191777956\n",
      "train loss:0.021739001406497684\n",
      "train loss:0.04925419574561592\n",
      "train loss:0.05212053035037695\n",
      "train loss:0.02160858940811756\n",
      "train loss:0.01845379995848737\n",
      "train loss:0.009997113365594546\n",
      "train loss:0.01573374842115558\n",
      "train loss:0.010727856531634446\n",
      "train loss:0.034826443502025614\n",
      "train loss:0.034502967120092924\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.953\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n=============== Final Test Accuracy ===============\\ntest acc:0.959\\n\\n전체로 학습했을 경우 약 98%까지 가능\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"\n",
    "    다음과 같은 CNN을 구성한다.\n",
    "    → Conv → ReLU → Pooling → Affine → ReLU → Affine → Softmax →\n",
    "    전체 구현은 simple_convnet.py 참고\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param={'filter_num': 30, 'filter_size': 5,\n",
    "                             'pad': 0, 'stride': 1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        # 초기화 인수로 주어진 하이퍼파라미터를 딕셔너리에서 꺼내고 출력 크기를 계산한다.\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / \\\n",
    "            filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) *\n",
    "                               (conv_output_size/2))\n",
    "\n",
    "        # 1층의 합성곱 계층과 2, 3층의 완전연결 계층의 가중치와 편향 생성\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # CNN을 구성하는 계층을 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],\n",
    "                                           self.params['b1'],\n",
    "                                           conv_param['stride'],\n",
    "                                           conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"추론을 수행\"\"\"\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실함수 값 계산\"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"오차역전파법으로 기울기를 구함\"\"\"\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "# 본 신경망으로 실제 MNIST 데이터셋을 학습하는 코드는 train_convnet.py 참고\n",
    "\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1, 28, 28),\n",
    "                        conv_param={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "=============== Final Test Accuracy ===============\n",
    "test acc:0.959\n",
    "\n",
    "전체로 학습했을 경우 약 98%까지 가능\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
